### Comprehensive Prompt for Thorough Bug Detection and Functionality Verification of a Website in a Local Environment

You are an expert quality assurance (QA) specialist and software tester with extensive experience in full-stack web development, specializing in e-commerce platforms like Shopify or BigCommerce. Your task is to create a super-detailed, exhaustive testing prompt that systematically identifies bugs, verifies complete functionality, and ensures the website is fully operational in a local development setup (e.g., running on http://localhost:3000 via Node.js, React, Express, or similar stacks). This prompt must adhere strictly to established testing rules and principles, including:

- **Testing Pyramid Structure**: Prioritize unit tests at the base, followed by integration tests, system/end-to-end tests, and exploratory/manual tests at the top.
- **Functional vs. Non-Functional Testing**: Cover functional aspects (what the system does) like user flows and features, and non-functional aspects (how it performs) like speed, security, and usability.
- **Black-Box and White-Box Testing**: Use black-box for user-facing behaviors without code knowledge, and white-box for internal logic inspection via dev tools or code reviews.
- **Positive and Negative Testing**: Validate expected successes (positive) and deliberate failures/errors (negative, e.g., invalid inputs).
- **Regression Testing**: Re-test after fixes to prevent bug reintroduction.
- **Boundary and Edge Case Testing**: Check extremes like maximum inputs, zero values, or unusual user behaviors.
- **Compatibility Testing**: Across browsers, devices, and screen sizes.
- **Accessibility Standards**: WCAG 2.1 compliance.
- **Performance Metrics**: Load times, resource usage.
- **Security Best Practices**: OWASP top 10 vulnerabilities.
- **Documentation and Reporting**: Log every test case with steps, expected/actual results, pass/fail, screenshots/logs, and severity (critical, high, medium, low).
- **Tools Integration**: Leverage browser dev tools (Chrome/Firefox), Postman for APIs, Lighthouse for audits, Jest/Cypress for automation, and manual inspection.
- **Assumptions**: The website is a typical e-commerce site (e.g., with products, user auth, carts, payments). Adapt to specifics if provided (e.g., AI features, AR integrations). Run in at least Chrome, Firefox, and one mobile emulator (e.g., Chrome DevTools device mode). No internet-dependent features unless mocked.

Structure your testing process into three distinct, sequential phases for logical progression: Phase 1 (Preparation and Setup Validation), Phase 2 (Core Testing Execution), and Phase 3 (Advanced Analysis, Reporting, and Iteration). Each phase must be super-detailed and lengthy, with numbered steps, sub-steps, specific examples, and quantifiable criteria (e.g., load time < 2 seconds). Aim for completeness: Cover 100+ test cases across categories. After execution, compile a final report in Markdown format with tables for test results, bug trackers (ID, description, reproduction steps, fix suggestions), and overall functionality score (e.g., 95% if minor bugs only).

#### Phase 1: Preparation and Setup Validation (Ensure Environment Readiness Before Testing)
This phase focuses on verifying the local setup to avoid false positives from environmental issues. Allocate 1-2 hours; document any setup failures as pre-test bugs.

1. **Server and Dependency Initialization**:
   - Run the startup command (e.g., `npm run dev` or `python manage.py runserver`) and monitor console output for 5 minutes. Expected: No errors, warnings, or unhandled exceptions; server binds to port (e.g., 3000) successfully. Actual: Log stdout/stderr. Fail if crashes occur—reproduce by checking package.json/scripts.
   - Verify dependencies: Execute `npm list` or `pip freeze` and cross-check against requirements.txt/package.json for missing/outdated packages (e.g., React >=18). Test for conflicts by isolating imports in a sample script.
   - Database/Storage Check: If applicable (e.g., MongoDB/SQLite), connect via CLI (e.g., `mongo` or `sqlite3 db.sqlite`), query test data (e.g., SELECT * FROM products LIMIT 5), and confirm seeding (e.g., at least 10 sample products). Negative test: Attempt connection with invalid credentials; expect graceful error.

2. **Configuration and Environment Variables**:
   - Inspect .env file: Ensure keys like DATABASE_URL, API_KEY are set to local/test values (no production leaks). Test by altering a key (e.g., wrong port) and verifying server fails to start—restore after.
   - Browser Access: Open http://localhost:3000 in Chrome and Firefox. Expected: Page loads without 404/500 errors. Check Network tab in dev tools for failed requests (e.g., missing assets).

3. **Mocking External Services**:
   - For APIs/integrations (e.g., payment gateways, AI engines): Use tools like Mockoon or JSON Server to mock endpoints. Example: Create a /api/products mock returning {id:1, name:'Test Product'}. Test by calling it in code and verifying responses.
   - Data Seeding: Populate with diverse test data (e.g., valid/invalid emails, edge prices like $0 or $99999.99).

4. **Tool Setup**:
   - Install/configure testing tools locally: Cypress (`npm i cypress`), Postman collections for APIs, Lighthouse via Chrome extension.
   - Baseline Audit: Run Lighthouse on homepage; aim for scores >90 in Performance, Accessibility, Best Practices, SEO. Log initial results as baseline.

5. **Initial Smoke Test**:
   - Navigate core pages (home, login, product list). Expected: No crashes, basic rendering. Negative: Force errors like disabling JS in browser; expect fallback messages.

#### Phase 2: Core Testing Execution (Systematic Bug Hunting Across Categories)
This phase executes 80+ detailed test cases in categories, following positive/negative, boundary, and regression rules. Use automation where possible (e.g., Cypress scripts for flows); manual for exploratory. Test in isolation, then integrated. Allocate 4-6 hours per category.

1. **Functional Testing (User Flows and Features)**:
   - **Authentication**: Positive: Register new user with valid email/password; login/logout. Expected: Session persists (cookies/localStorage). Negative: Invalid creds (e.g., short password <6 chars); expect error messages without exposing details (e.g., no "user exists"). Boundary: Max length fields (e.g., 255-char email). Regression: After logout, attempt protected routes; expect redirect.
   - **Product Management**: Browse/list products. Positive: Filter/sort (e.g., by price); add to cart. Negative: Add out-of-stock item; expect alert. Edge: 100+ items—check pagination. Integration: Cart updates reflect in totals.
   - **Checkout/Payment**: Simulate flow. Positive: Enter valid card (use test numbers like 4242424242424242); complete order. Negative: Invalid CVV; expect field highlight. Boundary: Zero items in cart—disable checkout button.
   - **Search and Navigation**: Positive: Search "test product"; results match. Negative: Empty/malformed query (e.g., SQL injection attempt like ' OR 1=1); expect sanitized/no results. Voice search mock: If implemented, test with URL params.

2. **UI/UX and Responsiveness Testing**:
   - **Rendering**: Inspect elements via dev tools. Positive: All components load (e.g., images alt-text present). Negative: Resize window to 320px; check for overflows. Compatibility: Test in Chrome, Firefox, Safari emulator—note CSS inconsistencies (e.g., flexbox bugs).
   - **Interactivity**: Click buttons, forms. Positive: Hover effects, animations (e.g., parallax scroll smooth). Negative: Rapid clicks; expect no double-submits (debounce check). Touch: Emulate mobile—swipe gestures work.
   - **Accessibility**: Use Axe DevTools: Check for ARIA labels on interactive elements, contrast ratios >4.5:1, keyboard navigation (tab order logical). Negative: Screen reader simulation (e.g., NVDA)—ensure readable errors.

3. **API and Backend Testing (White-Box Focus)**:
   - Use Postman: GET /api/products—expected 200 OK, JSON array. Negative: Invalid params (e.g., negative ID); 400 Bad Request. Rate limiting: Spam requests; expect 429 if implemented.
   - Integration: Chain calls (e.g., POST /login then GET /user)—verify auth tokens. Edge: Large payloads (e.g., 10MB image upload); expect rejection.
   - Database Interactions: White-box: Inspect queries in logs for efficiency (no N+1 issues). Test transactions: Simulate concurrent updates (e.g., two tabs editing stock); ensure consistency.

4. **Non-Functional Testing**:
   - **Performance**: Time page loads (dev tools Network tab)—<2s for homepage. Stress: Open 10 tabs; monitor CPU/RAM via Task Manager (<50% spike). Lazy loading: Scroll; images load on demand.
   - **Security**: Scan for OWASP: XSS—input <script>alert(1)</script>; expect escaped. CSRF: Check tokens in forms. SQL Injection: Input 'DROP TABLE;—expect no execution. HTTPS: If configured, enforce redirects.
   - **Error Handling**: Force 404 (invalid URL); custom page. Network offline: Service worker caches assets. Logging: Check console for uncaught errors.

5. **Exploratory and Edge Case Testing**:
   - Monkey Testing: Random inputs/actions (e.g., mash keys in forms). International: Non-ASCII chars (e.g., accents in names). Device: Emulate slow 3G; check timeouts.

#### Phase 3: Advanced Analysis, Reporting, and Iteration (Post-Test Evaluation and Fixes)
This phase analyzes results, prioritizes bugs, and iterates. Allocate 2-3 hours; repeat Phase 2 for regressions.

1. **Bug Classification and Prioritization**:
   - Assign IDs (e.g., BUG-001), severity (critical: crashes site; low: cosmetic). Examples: Critical—login fails; High—cart miscalculates; Medium—slow load; Low—typo.
   - Root Cause Analysis: Use dev tools debugger for JS errors, logs for backend.

2. **Reporting Format**:
   - Use tables: | Test Case ID | Description | Steps | Expected | Actual | Pass/Fail | Severity | Screenshots/Logs |
   - Summary: Total tests run, pass rate, functionality score (e.g., 85% if major bugs). Recommendations: Fix priorities, e.g., "Patch XSS in form inputs via DOMPurify."
   - Visuals: Attach screenshots (e.g., via Snipping Tool), video recordings for flows (e.g., Loom).

3. **Iteration and Regression**:
   - Fix Simulation: Note suggested code changes (e.g., add validation: if(input.length > 255) reject;).
   - Re-Test: Run failed cases post-fix; update report. Full Regression: Re-run 20% of passing tests.
   - Final Verdict: Declare "Fully Functional" only if 100% critical/high bugs fixed, >95% overall pass. Otherwise, list blockers.

Execute this prompt step-by-step, logging time per phase. If bugs found, halt deployment until resolved. This ensures adherence to testing rules for a bug-free, functional local website.